# -*- coding: utf-8 -*-
"""KNNipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qyhE0JtVKE2yZNNpFDzCx0pWdR1uUlZD
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd#veri görselleştirme
import numpy as np#lineer cebir vs 
import re# specifies a set of strings that matches
import seaborn as sns#veri görselleştirme
import matplotlib.pyplot as plt#tablo grafik vs
# %matplotlib inline
import warnings
from sklearn.model_selection import train_test_split#sklearn test ve deneme
from mlxtend.plotting import plot_decision_regions#desicion 
from sklearn.preprocessing import StandardScaler#normalizasyon
from sklearn.neighbors import KNeighborsClassifier#yakın komşu 
from sklearn.tree import DecisionTreeClassifier #karar agacı sınıflandırması
from sklearn.model_selection import train_test_split, cross_val_score#cross validation için test train
from sklearn.utils.multiclass import unique_labels #label
from sklearn import metrics #metric işlemleri için
import glob
import os
import matplotlib.pyplot as plt
import string

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score#tahmini değer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

data = pd.read_csv("dataset.csv") #verisetimiz eklendi

data.head() #ilk beş değer basıldı

data.dtypes.value_counts()#tür saydırma

data.info()
data.isna().sum()  #boş alan var mı

data.describe() #describe() metodu sayısal verilere sahip olan sütunların max, min , std… gibi istatiksel değerlerini döndürür

#Nedir? Label Encoding, kategorik değişkenleri işlemek için popüler bir kodlama tekniğidir
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
data['First_Name'] = label_encoder.fit_transform(data['First_Name'])
data["First_Name"].unique()

data['Gender'] = label_encoder.fit_transform(data['Gender'])
data["Gender"].unique()

data['Last_Name'] = label_encoder.fit_transform(data['Last_Name'])
data["Last_Name"].unique()

data['Agent_ID'] = label_encoder.fit_transform(data['Agent_ID'])
data["Agent_ID"].unique()

data['Area_Code'] = label_encoder.fit_transform(data['Area_Code'])
data["Area_Code"].unique()

data['Phone_code'] = label_encoder.fit_transform(data['Phone_code'])
data["Phone_code"].unique()

data['Sale'] = label_encoder.fit_transform(data['Sale'])
data['Sale'].unique()

print('sütun isimleri')
print(data.columns)

data[data.columns].nunique() #Belirtilen eksendeki farklı öğelerin sayısını sayın

plt.figure(figsize=(6,4))
sns.countplot(y=data.dtypes ,data=data)
plt.xlabel("Data Type saysı")
plt.ylabel("Data types")

X=data.iloc[:,1:] # "class" kolonu dışındaki tüm kolonlar girdi değeri olarak tanımlandı. Ne kadar çok birbiriyle alakalı girdi kolonu kullanılırsa çıktının tahmini o kadar yüksek doğrulukta çıkacaktır.

y=data.iloc[:,0]  # "class" kolonu çıktı değeri olarak tanımlandı. Çünkü class kolonu içerisindeki kategori tahmin edilecek.

plt.figure(figsize=(15,8))
sns.heatmap(data.corr(),cmap='magma',linecolor='white',linewidths=1,annot=True)

X = data[['Call_ID', 'Agent_ID', 'Age', 'Product_ID', 'Timezone', 'Phone_code', 
        'First_Name', 'Last_Name', 'Area_Code', 'Gender', 'Call_Count']].values

y = data['Sale'].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=4)

X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3) # Test Verisi %30 Eğitim Verisi %70 olarak atandı.

print("train set", X_train.shape, y_train.shape)
print("test set", X_test.shape, y_test.shape)

data.head()

g = sns.factorplot(x = "Sale", y = "Age", kind = "bar", data = data, size = 6)
g.set_ylabels("data")
plt.show()

p = data.hist(figsize = (20,20))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# plt.figure(figsize=(16,8))
# sns.countplot(x='Age', hue='Sale',data=data, palette="rainbow")
# plt.xticks(rotation=90)

data.dtypes

# Commented out IPython magic to ensure Python compatibility.
# %%time
# plt.figure(figsize=(16,8))
# sns.countplot(x='Sale',data=data,)
# plt.xticks(rotation=90)

from sklearn.neighbors import KNeighborsClassifier # KNN Algoritmasının modülü projeye dahil edildi.

KModel=KNeighborsClassifier(n_neighbors=30,metric="minkowski") # KNN Modeli kuruldu ve k komşuluk sayısı 30 olarak alındı. Yani sınıflandırılacak olan verinin 30 eleman komşuluğuna bakara karar verir. 
# Eğer herhangi bir uzaklık metriği verilmez ise algoritma "minkowski" uzaklığına göre uzaklık hesaplayacaktır. Burada da minkowski uzaklık metriği kullanılmıştır.

KModel.fit(X_train,y_train) # KNN algoritması ayarlanan Eğitim verileri üzerinde uygulandı ve model "Eğitildi"

y_pred=KModel.predict(X_test) # Uygulanan model için Test verileri tahmin edildi.

print(len(X_train))

#matematiksel olarak normalizasyon işlemi

x_norm = (X - np.min(X))/(np.max(X) - np.min(X))

print("NORMALİZASYON İŞLEMİ ÖNCESİ:","\nMin :")
  
print(np.min(X))
print("\nMax :")
print(np.max(X))


print("\n\nNORMALİZASYON İŞLEMİ SONRASI:","\nMin :")
      
print(np.min(x_norm))
print("\nMax :")
print(np.max(x_norm))

#minmax scaler ile verimizi normalizasyon işleminden geçirdik çünkü knn yanı yakın komşuyu bu şekilde aramak daha doğru sonuç vermesini sağlar
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train_normed = mms.fit_transform(X_train) 
X_test_normed= mms.transform(X_test)
print(X_test)
print(X_train)

#verimizi standarilazzyon işleminden geçirdik
from sklearn.preprocessing import StandardScaler
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
print(X_test)
print(X_train)

# sklearn'den KNeighbors Classifier'ı içe aktarma işlemi
from sklearn.neighbors import KNeighborsClassifier


# modeli somutlaştırma işlemi
knn = KNeighborsClassifier(n_neighbors=3)


# # modeli eğitim setine uydurma
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test) #tahmini işlemi dödür kategorik yanı array türünde

y_pred

#olasılıkların hesaplanacağı verilere karşılık gelen tek bir bağımsız değişkeni kabul eder ve girdi veri noktaları için
# sınıf olasılıklarını içeren bir dizi liste döndürür.
knn.predict_proba(X_test)[:,0]

knn.predict_proba(X_test)[:,1]

from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

y_pred_train = knn.predict(X_train)

print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

# modeli k=5 ile başlatma
knn_5 = KNeighborsClassifier(n_neighbors=5)


# modeli eğitim setine uydurma
knn_5.fit(X_train, y_train)


# test setinde tahmin ettirme
y_pred_5 = knn_5.predict(X_test)


print('k=5 ile model doğruluk puanı: {0:0.4f}'. format(accuracy_score(y_test, y_pred_5)))

# modeli k=7 ile başlatma
knn_7 = KNeighborsClassifier(n_neighbors=7)


knn_7.fit(X_train, y_train)


#test setinde tahmin ettirme
y_pred_7 = knn_7.predict(X_test)


print('k=7 ile model doğruluk puanı : {0:0.4f}'. format(accuracy_score(y_test, y_pred_7)))

# k=8
knn_8 = KNeighborsClassifier(n_neighbors=8)


knn_8.fit(X_train, y_train)



y_pred_8 = knn_8.predict(X_test)


print('k=8 ile model doğruluk puanı : {0:0.4f}'. format(accuracy_score(y_test, y_pred_8)))

#  k=9
knn_9 = KNeighborsClassifier(n_neighbors=9)


knn_9.fit(X_train, y_train)



y_pred_9 = knn_9.predict(X_test)


print('k=9 ile model doğruluk puanı : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))

# %% Buradaki fonksiyon 2 nokta arasındaki uzaklığı hesaplamaktadır.
# Distance = Sqrt(Sum((p1-p2)^2)) 
#
def Distance(point_1,point_2):
    total = 0
    for idx in range(len(point_1)):
        total = total + (point_1[idx] - point_2[idx])**2
    return total**0.5

def K_NNeighbors(k_value, x_train, y_train, x_test):
    y_predict = []
    
    #Herbir test noktası için diğer tüm noktalara olan uzaklıklar hesaplanıyor.
    #Bulunan uzaklıklar etiketlerle beraber 'Neighbors' değişkeninde tutuluyor.
    for idx_test in range(x_test.shape[0]):
        Neighbors = []
        test_point = x_test[idx_test]
        for idx_rows in range(x_train.shape[0]):
            train_point = x_train[idx_rows]
            Neighbors.append([Distance(test_point, train_point),y_train[idx_rows]])
        
        # Her bir komşunun test noktasına olan uzaklığı bulunuyor.En yakın 'K' tane komşuyu seçmek için 
        # öncelikle komşular uzaklıklarına göre küçükten büyüğe doğru sıralanıyor..
        # Daha sonra k tane komşu seçilip içerisinden etiket(label) değerleri çekiliyor.
        Neighbors.sort()
        Neighbors = Neighbors[0:k_value]
        Labels = [n[1] for n in Neighbors]
        
        # En yakın k tane komşunun sahip olduğu etiketlerin frekansları bulunuyor ve en yüksek frekansa sahip
        # etiket test noktasını sınıflamakta kullanılıyor.
        from itertools import groupby
        Freq = [[len(list(group)), key] for key, group in groupby(Labels)]
        y_predict.append(max(Freq)[1])
    return y_predict

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

precision = metrics.accuracy_score(y_pred, y_test) * 100
print("Accuracy with K-NN: {0:.2f}%".format(precision))

from sklearn.neighbors import KNeighborsRegressor
regressor = KNeighborsRegressor(n_neighbors=5)
regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print(f'mae: {mae}')
print(f'mse: {mse}')
print(f'rmse: {rmse}')

regressor.score(X_test, y_test)

error = []

#k değeri 1-39 arasında değer girdisi sağlar
for i in range(1, 40):
    knn = KNeighborsRegressor(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    mae = mean_absolute_error(y_test, pred_i)
    error.append(mae)

import matplotlib.pyplot as plt 

plt.figure(figsize=(12, 6))
plt.plot(range(1, 40), error, color='red', 
         linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
         
plt.title('K değeri MAE')
plt.xlabel('K değeri')
plt.ylabel('Mean Absolute Error')

from sklearn.metrics import confusion_matrix
sale= confusion_matrix(y_test, y_pred)
print(sale)

#1-10 arasındaki durumlarını inceleyelim
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #model tahmin ettirme
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)

    
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc

#*Accuracy ile K değeri arasındaki ilişkinin grafiğini çizelim*
plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()

print( "En yüksek Doğruluk=", mean_acc.max(), "K=", mean_acc.argmax()+1,"olduğunda gerçekleşti.")